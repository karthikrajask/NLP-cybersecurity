{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a49de10d-783f-4ed0-9349-2198c821aa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error translating text: sequence item 0: expected str instance, NoneType found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('C:/Users/hr893/Downloads/top500_updated.csv')\n",
    "\n",
    "# Initialize translator\n",
    "translator = Translator()\n",
    "\n",
    "# Define a function to translate text\n",
    "def translate_text(text):\n",
    "    if pd.isna(text):  # Check if the entry is None or NaN\n",
    "        return \"\"\n",
    "    try:\n",
    "        # Translate text to English\n",
    "        return translator.translate(text, dest='en').text\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {e}\")\n",
    "        return text  # Return original if translation fails\n",
    "\n",
    "# Apply the translation function to the \"crimeadditionalinfo\" column\n",
    "data['crimeaditionalinfo'] = data['crimeaditionalinfo'].apply(translate_text)\n",
    "\n",
    "# Save the updated dataframe\n",
    "data.to_csv('D:/Downloads/sample500_english2.csv', index=False)\n",
    "print(\"Translation complete. The file has been saved as 'translated_sample100.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34b7ac38-59ec-467b-8d8e-2bef4b713d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  crimeaditionalinfo  \\\n",
      "0  I had continue received random calls and abusi...   \n",
      "1  The above fraudster is continuously messaging ...   \n",
      "2  He is acting like a police and demanding for m...   \n",
      "3  In apna Job I have applied for job interview f...   \n",
      "4  I received a call from lady stating that she w...   \n",
      "\n",
      "                                processed_crime_info  \n",
      "0  continue received random calls abusive message...  \n",
      "1  fraudster continuously messaging asking pay mo...  \n",
      "2  acting like police demanding money adding sect...  \n",
      "3  apna job applied job interview telecalling res...  \n",
      "4  received call lady stating send new phone vivo...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hr893\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('D:/Downloads/sample500_english1.csv')\n",
    "\n",
    "# Download stopwords if you haven't done so\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Create a set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function for advanced text preprocessing\n",
    "def advanced_preprocess_text(text):\n",
    "    # Check if the text is valid\n",
    "    if isinstance(text, str):\n",
    "        # Lowercase the text\n",
    "        text = text.lower()\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b', '', text)\n",
    "        # Remove phone numbers\n",
    "        text = re.sub(r'\\+?\\d[\\d -]{8,12}\\d', '', text)  # Adjust regex for phone numbers as necessary\n",
    "        # Remove special characters and numbers, keeping only alphabetic characters\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        # Tokenize and filter out short words and stopwords\n",
    "        tokens = [word for word in text.split() if len(word) > 1 and word not in stop_words]\n",
    "        # Join tokens back to a single string\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        return ''  # Return an empty string for non-text entries\n",
    "\n",
    "# Apply the advanced preprocessing function\n",
    "data['processed_crime_info'] = data['crimeaditionalinfo'].apply(advanced_preprocess_text)\n",
    "\n",
    "# Display the first few rows to check the preprocessing\n",
    "print(data[['crimeaditionalinfo', 'processed_crime_info']].head())\n",
    "data.to_csv('D:/Downloads/sample500_english2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f464a460-a973-447a-9d8b-b89832e37576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('D:/Downloads/sample500_english2.csv')\n",
    "\n",
    "# Select relevant columns and drop any NaN rows\n",
    "data = data[['processed_crime_info', 'category', 'sub-category']].dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "203e4bee-e8a9-4233-948e-617d01938518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder_cat = LabelEncoder()\n",
    "label_encoder_subcat = LabelEncoder()\n",
    "data['category_label'] = label_encoder_cat.fit_transform(data['category'])\n",
    "data['sub_category_label'] = label_encoder_subcat.fit_transform(data['sub-category'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c1dba7e-b731-4fc9-a7da-2f0bac74222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train_cat, y_test_cat, y_train_subcat, y_test_subcat = train_test_split(\n",
    "    data['processed_crime_info'], \n",
    "    data['category_label'], \n",
    "    data['sub_category_label'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "743dd8b3-2d92-4a63-bfc0-6fe3edcc834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Dataset class for PyTorch DataLoader\n",
    "class CrimeDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        inputs = self.tokenizer(text, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        inputs['labels'] = label\n",
    "        return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dededd10-d522-4f84-abce-9eccc1570815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load BERT models for sequence classification (one for each target)\n",
    "model_cat = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder_cat.classes_))\n",
    "model_subcat = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder_subcat.classes_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "560e809c-9676-4d61-a24e-6d9da9c6dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc2736ad-300b-4e25-976f-721a65f78151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric function for evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {'accuracy': acc, 'f1': f1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2e2c38d-ebd6-4aa9-84a3-2b24e3623934",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train_cat = y_train_cat.reset_index(drop=True).tolist()\n",
    "y_test_cat = y_test_cat.reset_index(drop=True).tolist()\n",
    "y_train_subcat = y_train_subcat.reset_index(drop=True).tolist()\n",
    "y_test_subcat = y_test_subcat.reset_index(drop=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef2a0031-aa99-4391-9ef6-b515f3dc4797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the datasets\n",
    "train_dataset_cat = CrimeDataset(X_train, y_train_cat)\n",
    "test_dataset_cat = CrimeDataset(X_test, y_test_cat)\n",
    "train_dataset_subcat = CrimeDataset(X_train, y_train_subcat)\n",
    "test_dataset_subcat = CrimeDataset(X_test, y_test_subcat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b78ecd5-cfc8-4716-8840-c00ef5cd6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer for category\n",
    "trainer_cat = Trainer(\n",
    "    model=model_cat,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_cat,\n",
    "    eval_dataset=test_dataset_cat,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Trainer for sub_category\n",
    "trainer_subcat = Trainer(\n",
    "    model=model_subcat,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_subcat,\n",
    "    eval_dataset=test_dataset_subcat,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1912b0df-a862-4c9b-a8f4-1e3932763f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Category Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='495' max='495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [495/495 30:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.188440</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.508608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.071212</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.674196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.056067</td>\n",
       "      <td>0.686869</td>\n",
       "      <td>0.646713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.278542</td>\n",
       "      <td>0.676768</td>\n",
       "      <td>0.655129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.310532</td>\n",
       "      <td>0.717172</td>\n",
       "      <td>0.670096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sub-category Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='495' max='495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [495/495 31:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.547632</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.176153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.098235</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.336918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.007906</td>\n",
       "      <td>0.474747</td>\n",
       "      <td>0.421667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.934380</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.409056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.908747</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.462578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category classification evaluation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3105322122573853, 'eval_accuracy': 0.7171717171717171, 'eval_f1': 0.6700962325962326, 'eval_runtime': 20.7079, 'eval_samples_per_second': 4.781, 'eval_steps_per_second': 1.207, 'epoch': 5.0}\n",
      "\n",
      "Sub-category classification evaluation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.908746600151062, 'eval_accuracy': 0.5151515151515151, 'eval_f1': 0.46257753938233875, 'eval_runtime': 20.2321, 'eval_samples_per_second': 4.893, 'eval_steps_per_second': 1.236, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Training the models\n",
    "print(\"Training Category Model...\")\n",
    "trainer_cat.train()\n",
    "print(\"Training Sub-category Model...\")\n",
    "trainer_subcat.train()\n",
    "\n",
    "# Evaluate the models\n",
    "print(\"\\nCategory classification evaluation:\")\n",
    "cat_eval_results = trainer_cat.evaluate()\n",
    "print(cat_eval_results)\n",
    "\n",
    "print(\"\\nSub-category classification evaluation:\")\n",
    "subcat_eval_results = trainer_subcat.evaluate()\n",
    "print(subcat_eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8384ad57-27ab-4b16-aa90-775cada0e56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and label encoders saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the models\n",
    "model_cat.save_pretrained('D:/Downloads/category_model')\n",
    "model_subcat.save_pretrained('D:/Downloads/subcategory_model')\n",
    "\n",
    "# Save the label encoders\n",
    "import joblib\n",
    "\n",
    "joblib.dump(label_encoder_cat, 'category_label_encoder.pkl')\n",
    "joblib.dump(label_encoder_subcat, 'subcategory_label_encoder.pkl')\n",
    "\n",
    "print(\"Models and label encoders saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e677ddb-6e84-439b-aee3-6cdaff0af246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: Online and Social Media Related Crime\n",
      "Predicted Sub-category: Cyber Bullying  Stalking  Sexting\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import joblib\n",
    "\n",
    "# Load the models\n",
    "model_cat = BertForSequenceClassification.from_pretrained('D:/Downloads/category_model')\n",
    "model_subcat = BertForSequenceClassification.from_pretrained('D:/Downloads/subcategory_model')\n",
    "\n",
    "# Load the label encoders\n",
    "label_encoder_cat = joblib.load('category_label_encoder.pkl')\n",
    "label_encoder_subcat = joblib.load('subcategory_label_encoder.pkl')\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to preprocess input text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase the text\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters and numbers\n",
    "    tokens = [word for word in text.split() if len(word) > 1]  # Filter out short words\n",
    "    return ' '.join(tokens)  # Join tokens back to a single string\n",
    "\n",
    "# Input text\n",
    "input_text = \"\"\"\n",
    "He was called from Maharashtra and created fake video of mine and demanding money from me or else saying that he will post that fake video in internet facebook youtube telegram He demanding many thousands of rupees\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Preprocess the input text\n",
    "processed_text = preprocess_text(input_text)\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(processed_text, padding='max_length', truncation=True, return_tensors='pt', max_length=128)\n",
    "\n",
    "# Make predictions for category\n",
    "model_cat.eval()\n",
    "with torch.no_grad():\n",
    "    category_logits = model_cat(**inputs).logits\n",
    "\n",
    "# Make predictions for sub-category\n",
    "model_subcat.eval()\n",
    "with torch.no_grad():\n",
    "    subcategory_logits = model_subcat(**inputs).logits\n",
    "\n",
    "# Get predicted category and sub-category labels\n",
    "predicted_category_index = torch.argmax(category_logits, dim=1).item()\n",
    "predicted_subcategory_index = torch.argmax(subcategory_logits, dim=1).item()\n",
    "\n",
    "# Map indices back to labels\n",
    "predicted_category = label_encoder_cat.inverse_transform([predicted_category_index])[0]\n",
    "predicted_subcategory = label_encoder_subcat.inverse_transform([predicted_subcategory_index])[0]\n",
    "\n",
    "# Print the predictions\n",
    "print(f\"Predicted Category: {predicted_category}\")\n",
    "print(f\"Predicted Sub-category: {predicted_subcategory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58026b4f-f9c8-4e33-a43d-40930cce9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Karthick\n",
      "[nltk_data]     Raja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nNo module named 'keras.engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1146\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1206\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1149\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:38\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     29\u001b[0m     TFBaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     TFTokenClassifierOutput,\n\u001b[0;32m     37\u001b[0m )\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     39\u001b[0m     TFCausalLanguageModelingLoss,\n\u001b[0;32m     40\u001b[0m     TFMaskedLanguageModelingLoss,\n\u001b[0;32m     41\u001b[0m     TFModelInputType,\n\u001b[0;32m     42\u001b[0m     TFMultipleChoiceLoss,\n\u001b[0;32m     43\u001b[0m     TFNextSentencePredictionLoss,\n\u001b[0;32m     44\u001b[0m     TFPreTrainedModel,\n\u001b[0;32m     45\u001b[0m     TFQuestionAnsweringLoss,\n\u001b[0;32m     46\u001b[0m     TFSequenceClassificationLoss,\n\u001b[0;32m     47\u001b[0m     TFTokenClassificationLoss,\n\u001b[0;32m     48\u001b[0m     get_initializer,\n\u001b[0;32m     49\u001b[0m     keras_serializable,\n\u001b[0;32m     50\u001b[0m     unpack_inputs,\n\u001b[0;32m     51\u001b[0m )\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shape_list, stable_softmax\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:69\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_adapter\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasTensor\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.engine'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 97\u001b[0m\n\u001b[0;32m     94\u001b[0m num_category_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(train_labels_category))\n\u001b[0;32m     95\u001b[0m num_sub_category_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(train_labels_sub_category))\n\u001b[1;32m---> 97\u001b[0m category_model \u001b[38;5;241m=\u001b[39m \u001b[43mTFAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_category_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m sub_category_model \u001b[38;5;241m=\u001b[39m TFAutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, num_labels\u001b[38;5;241m=\u001b[39mnum_sub_category_labels)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Compile the models with Adam optimizer and sparse categorical crossentropy\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:470\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    467\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    468\u001b[0m     )\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m--> 470\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    472\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    473\u001b[0m     )\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:360\u001b[0m, in \u001b[0;36m_get_model_class\u001b[1;34m(config, model_mapping)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[1;32m--> 360\u001b[0m     supported_models \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:602\u001b[0m, in \u001b[0;36m_LazyAutoMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping:\n\u001b[0;32m    601\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[model_type]\n\u001b[1;32m--> 602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m model_types \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m==\u001b[39m key\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:616\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[1;34m(self, model_type, attr)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m    615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name] \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 616\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:561\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[1;34m(module, attr)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[1;32m--> 561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, attr):\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1136\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1134\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1136\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1137\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1148\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1150\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1151\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nNo module named 'keras.engine'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('top500_updated.csv')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set of stop words in English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Preprocessing\n",
    "# Drop rows with missing values in essential columns (e.g., 'crimeaditionalinfo')\n",
    "data.dropna(subset=['crimeaditionalinfo', 'category', 'sub-category'], inplace=True)\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "data['crimeaditionalinfo'] = data['crimeaditionalinfo'].apply(clean_text)\n",
    "\n",
    "# Encode categorical labels\n",
    "label_encoder_category = LabelEncoder()\n",
    "label_encoder_sub_category = LabelEncoder()\n",
    "data['category'] = label_encoder_category.fit_transform(data['category'])\n",
    "data['sub-category'] = label_encoder_sub_category.fit_transform(data['sub-category'])\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define function to tokenize text data\n",
    "def tokenize_text(data, max_length=128):\n",
    "    return tokenizer(\n",
    "        data['crimeaditionalinfo'].tolist(),\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "# Tokenize the training and validation data\n",
    "train_encodings = tokenize_text(train_data)\n",
    "val_encodings = tokenize_text(val_data)\n",
    "\n",
    "# Prepare labels\n",
    "train_labels_category = train_data['category'].values\n",
    "train_labels_sub_category = train_data['sub-category'].values\n",
    "val_labels_category = val_data['category'].values\n",
    "val_labels_sub_category = val_data['sub-category'].values\n",
    "\n",
    "# Compute class weights for handling class imbalance\n",
    "category_class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(train_labels_category),\n",
    "    y=train_labels_category\n",
    ")\n",
    "sub_category_class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(train_labels_sub_category),\n",
    "    y=train_labels_sub_category\n",
    ")\n",
    "\n",
    "category_class_weights_dict = dict(enumerate(category_class_weights))\n",
    "sub_category_class_weights_dict = dict(enumerate(sub_category_class_weights))\n",
    "\n",
    "# Load the BERT model for category and sub-category classification\n",
    "num_category_labels = len(np.unique(train_labels_category))\n",
    "num_sub_category_labels = len(np.unique(train_labels_sub_category))\n",
    "\n",
    "category_model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_category_labels)\n",
    "sub_category_model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_sub_category_labels)\n",
    "\n",
    "# Compile the models with Adam optimizer and sparse categorical crossentropy\n",
    "optimizer = Adam(learning_rate=2e-5)  # Adjust learning rate if needed\n",
    "\n",
    "category_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "sub_category_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the category model\n",
    "print(\"Training Category Model...\")\n",
    "history_category = category_model.fit(\n",
    "    train_encodings['input_ids'], train_labels_category,\n",
    "    validation_data=(val_encodings['input_ids'], val_labels_category),\n",
    "    batch_size=16,  # Adjust based on your GPU capacity\n",
    "    epochs=5,\n",
    "    class_weight=category_class_weights_dict,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Train the sub-category model\n",
    "print(\"Training Sub-Category Model...\")\n",
    "history_sub_category = sub_category_model.fit(\n",
    "    train_encodings['input_ids'], train_labels_sub_category,\n",
    "    validation_data=(val_encodings['input_ids'], val_labels_sub_category),\n",
    "    batch_size=16,  # Adjust based on your GPU capacity\n",
    "    epochs=5,\n",
    "    class_weight=sub_category_class_weights_dict,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the category model\n",
    "print(\"Evaluating Category Model...\")\n",
    "category_eval = category_model.evaluate(val_encodings['input_ids'], val_labels_category)\n",
    "print(\"Category Model Evaluation:\", category_eval)\n",
    "\n",
    "# Evaluate the sub-category model\n",
    "print(\"Evaluating Sub-Category Model...\")\n",
    "sub_category_eval = sub_category_model.evaluate(val_encodings['input_ids'], val_labels_sub_category)\n",
    "print(\"Sub-Category Model Evaluation:\", sub_category_eval)\n",
    "\n",
    "# Print model evaluation metrics for each model\n",
    "print(\"Final Category Model Evaluation:\")\n",
    "print(f\"Loss: {category_eval[0]}, Accuracy: {category_eval[1]}\")\n",
    "\n",
    "print(\"Final Sub-Category Model Evaluation:\")\n",
    "print(f\"Loss: {sub_category_eval[0]}, Accuracy: {sub_category_eval[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b58f6b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U transformers tensorflow keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d515aedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \n",
      "The class this function is called from is 'MBartTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "# Fix seed for reproducibility\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Load the mBART model and tokenizer\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Function to translate text\n",
    "def translate(text):\n",
    "    try:\n",
    "        # Detect the language of the input text\n",
    "        detected_lang = detect(text)\n",
    "\n",
    "        # Map detected language to mBART source language codes\n",
    "        lang_mapping = {\n",
    "            'en': 'en_XX',  # English\n",
    "            'hi': 'hi_IN',  # Hindi\n",
    "            'es': 'es_XX',  # Spanish\n",
    "            'fr': 'fr_XX',  # French\n",
    "            'de': 'de_DE',  # German\n",
    "            'ja': 'ja_XX',  # Japanese\n",
    "            # Add other mappings as necessary\n",
    "        }\n",
    "\n",
    "        # Check if the detected language is supported\n",
    "        if detected_lang in lang_mapping:\n",
    "            source_lang = lang_mapping[detected_lang]\n",
    "        else:\n",
    "            return f\"Language '{detected_lang}' is not supported for translation.\"\n",
    "        \n",
    "        # Set the target language to English\n",
    "        target_lang = 'en_XX'\n",
    "\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", src_lang=source_lang)\n",
    "\n",
    "        # Generate translation\n",
    "        translated_tokens = model.generate(inputs[\"input_ids\"], forced_bos_token_id=tokenizer.lang2id[target_lang])\n",
    "\n",
    "        # Decode the translated tokens\n",
    "        return tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Load the CSV file\n",
    "input_file_path = \"top2000.csv\"  # Replace with your input CSV file path\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Ensure the CSV has the 'text' column for translation\n",
    "if 'crimeaditionalinfo' not in df.columns:\n",
    "    raise ValueError(\"CSV must contain a 'text' column.\")\n",
    "\n",
    "# Translate only the 'text' column\n",
    "df['translated_text'] = df['crimeaditionalinfo'].apply(translate)\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "output_file_path = \"translated_texts.csv\"  # Path for the output CSV file\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Translations saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21dbdd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     - ----------------------------------- 30.7/981.5 kB 660.6 kB/s eta 0:00:02\n",
      "     - ----------------------------------- 30.7/981.5 kB 660.6 kB/s eta 0:00:02\n",
      "     - ----------------------------------- 30.7/981.5 kB 660.6 kB/s eta 0:00:02\n",
      "     ---- ------------------------------- 112.6/981.5 kB 656.4 kB/s eta 0:00:02\n",
      "     ---- ------------------------------- 122.9/981.5 kB 599.1 kB/s eta 0:00:02\n",
      "     ------- ---------------------------- 194.6/981.5 kB 737.3 kB/s eta 0:00:02\n",
      "     -------- --------------------------- 235.5/981.5 kB 801.7 kB/s eta 0:00:01\n",
      "     -------- --------------------------- 235.5/981.5 kB 801.7 kB/s eta 0:00:01\n",
      "     ---------- ------------------------- 286.7/981.5 kB 737.3 kB/s eta 0:00:01\n",
      "     ----------- ------------------------ 317.4/981.5 kB 785.7 kB/s eta 0:00:01\n",
      "     ------------- ---------------------- 368.6/981.5 kB 791.2 kB/s eta 0:00:01\n",
      "     -------------- --------------------- 389.1/981.5 kB 781.2 kB/s eta 0:00:01\n",
      "     ---------------- ------------------- 450.6/981.5 kB 805.0 kB/s eta 0:00:01\n",
      "     ----------------- ------------------ 471.0/981.5 kB 818.5 kB/s eta 0:00:01\n",
      "     ------------------- ---------------- 522.2/981.5 kB 819.2 kB/s eta 0:00:01\n",
      "     -------------------- --------------- 563.2/981.5 kB 822.9 kB/s eta 0:00:01\n",
      "     ----------------------- ------------ 645.1/981.5 kB 883.0 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 665.6/981.5 kB 891.6 kB/s eta 0:00:01\n",
      "     ------------------------- ---------- 696.3/981.5 kB 843.9 kB/s eta 0:00:01\n",
      "     -------------------------- --------- 727.0/981.5 kB 833.5 kB/s eta 0:00:01\n",
      "     ---------------------------- ------- 778.2/981.5 kB 862.0 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 798.7/981.5 kB 855.2 kB/s eta 0:00:01\n",
      "     ------------------------------- ---- 860.2/981.5 kB 862.9 kB/s eta 0:00:01\n",
      "     -------------------------------- --- 890.9/981.5 kB 854.2 kB/s eta 0:00:01\n",
      "     -----------------------------------  972.8/981.5 kB 905.5 kB/s eta 0:00:01\n",
      "     ------------------------------------ 981.5/981.5 kB 887.8 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\karthick raja\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993254 sha256=1b0824838ba90ee434ae54a7dcb6d7dfcb18a854c786cf6e05cb44efe81cb09d\n",
      "  Stored in directory: c:\\users\\karthick raja\\appdata\\local\\pip\\cache\\wheels\\0a\\f2\\b2\\e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Karthick Raja\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8599dd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/991.5 kB 660.6 kB/s eta 0:00:02\n",
      "   - ------------------------------------- 41.0/991.5 kB 495.5 kB/s eta 0:00:02\n",
      "   -- ------------------------------------ 61.4/991.5 kB 469.7 kB/s eta 0:00:02\n",
      "   -- ------------------------------------ 71.7/991.5 kB 435.7 kB/s eta 0:00:03\n",
      "   --- ---------------------------------- 102.4/991.5 kB 454.0 kB/s eta 0:00:02\n",
      "   ---- --------------------------------- 122.9/991.5 kB 450.6 kB/s eta 0:00:02\n",
      "   ----- -------------------------------- 133.1/991.5 kB 413.7 kB/s eta 0:00:03\n",
      "   ----- -------------------------------- 153.6/991.5 kB 437.1 kB/s eta 0:00:02\n",
      "   ------ ------------------------------- 174.1/991.5 kB 456.4 kB/s eta 0:00:02\n",
      "   ------- ------------------------------ 184.3/991.5 kB 412.5 kB/s eta 0:00:02\n",
      "   ------- ------------------------------ 204.8/991.5 kB 445.2 kB/s eta 0:00:02\n",
      "   -------- ----------------------------- 215.0/991.5 kB 409.6 kB/s eta 0:00:02\n",
      "   --------- ---------------------------- 235.5/991.5 kB 424.3 kB/s eta 0:00:02\n",
      "   ---------- --------------------------- 266.2/991.5 kB 420.4 kB/s eta 0:00:02\n",
      "   ---------- --------------------------- 286.7/991.5 kB 431.3 kB/s eta 0:00:02\n",
      "   ------------ ------------------------- 317.4/991.5 kB 447.2 kB/s eta 0:00:02\n",
      "   ------------ ------------------------- 317.4/991.5 kB 447.2 kB/s eta 0:00:02\n",
      "   ------------- ------------------------ 348.2/991.5 kB 432.7 kB/s eta 0:00:02\n",
      "   -------------- ----------------------- 378.9/991.5 kB 454.0 kB/s eta 0:00:02\n",
      "   ---------------- --------------------- 419.8/991.5 kB 476.8 kB/s eta 0:00:02\n",
      "   ----------------- -------------------- 450.6/991.5 kB 469.9 kB/s eta 0:00:02\n",
      "   ------------------ ------------------- 481.3/991.5 kB 478.7 kB/s eta 0:00:02\n",
      "   ------------------- ------------------ 512.0/991.5 kB 486.7 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 532.5/991.5 kB 498.8 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 563.2/991.5 kB 491.5 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 614.4/991.5 kB 515.5 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 645.1/991.5 kB 520.8 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 675.8/991.5 kB 525.8 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 696.3/991.5 kB 535.6 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 727.0/991.5 kB 533.3 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 757.8/991.5 kB 543.6 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 757.8/991.5 kB 543.6 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 788.5/991.5 kB 529.8 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 809.0/991.5 kB 532.5 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 839.7/991.5 kB 536.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 870.4/991.5 kB 534.6 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 911.4/991.5 kB 549.1 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 921.6/991.5 kB 545.1 kB/s eta 0:00:01\n",
      "   -------------------------------------  972.8/991.5 kB 550.0 kB/s eta 0:00:01\n",
      "   -------------------------------------  983.0/991.5 kB 546.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- 991.5/991.5 kB 536.8 kB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Karthick Raja\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37a9a663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\karthick raja\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.28.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\karthick raja\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\karthick raja\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\karthick raja\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\karthick raja\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\karthick raja\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: requests in c:\\users\\karthick raja\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Using cached tokenizers-0.20.2-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\karthick raja\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\karthick raja\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\karthick raja\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\karthick raja\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karthick raja\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\karthick raja\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karthick raja\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Using cached transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "   ---------------------------------------- 0.0/447.5 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 61.4/447.5 kB 3.2 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 71.7/447.5 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 122.9/447.5 kB 1.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 194.6/447.5 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 256.0/447.5 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 256.0/447.5 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 317.4/447.5 kB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 389.1/447.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 389.1/447.5 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------  440.3/447.5 kB 981.9 kB/s eta 0:00:01\n",
      "   -------------------------------------  440.3/447.5 kB 981.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- 447.5/447.5 kB 874.6 kB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "   ---------------------------------------- 0.0/286.0 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/286.0 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 61.4/286.0 kB 1.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 122.9/286.0 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 204.8/286.0 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 225.3/286.0 kB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- -- 266.2/286.0 kB 962.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 276.5/286.0 kB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- 286.0/286.0 kB 840.6 kB/s eta 0:00:00\n",
      "Downloading tokenizers-0.20.2-cp311-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.4 MB 653.6 kB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.1/2.4 MB 930.9 kB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.2/2.4 MB 984.6 kB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.2/2.4 MB 980.4 kB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.2/2.4 MB 1.0 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.3/2.4 MB 1.0 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.3/2.4 MB 967.8 kB/s eta 0:00:03\n",
      "   ------ --------------------------------- 0.4/2.4 MB 955.7 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.4/2.4 MB 994.9 kB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.5/2.4 MB 1.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.5/2.4 MB 1.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.6/2.4 MB 1.0 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.6/2.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.7/2.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.8/2.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.8/2.4 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.8/2.4 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.8/2.4 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.9/2.4 MB 1.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.9/2.4 MB 1.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 1.0/2.4 MB 1.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 1.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.1/2.4 MB 992.2 kB/s eta 0:00:02\n",
      "   ------------------ --------------------- 1.1/2.4 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.2/2.4 MB 1.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.2/2.4 MB 1.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.3/2.4 MB 1.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.3/2.4 MB 1.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.4/2.4 MB 1.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.4/2.4 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.5/2.4 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.5/2.4 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.6/2.4 MB 1.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.6/2.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.7/2.4 MB 1.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.7/2.4 MB 1.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.4 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.9/2.4 MB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.0/2.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.0/2.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.2/2.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.2/2.4 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.3/2.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "   ---------------------------------------- 0.0/179.6 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 41.0/179.6 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 102.4/179.6 kB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 143.4/179.6 kB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 174.1/179.6 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- 179.6/179.6 kB 984.9 kB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.14.1\n",
      "    Uninstalling huggingface-hub-0.14.1:\n",
      "      Successfully uninstalled huggingface-hub-0.14.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.28.1\n",
      "    Uninstalling transformers-4.28.1:\n",
      "      Successfully uninstalled transformers-4.28.1\n",
      "Successfully installed fsspec-2024.10.0 huggingface-hub-0.26.2 safetensors-0.4.5 tokenizers-0.20.2 transformers-4.46.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Karthick Raja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Karthick Raja\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec655d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
